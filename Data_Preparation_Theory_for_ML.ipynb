{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ea724ba-7cce-45cc-8c3c-8c0e994556ef",
   "metadata": {},
   "source": [
    "## Exploratory Data analysis\n",
    "\n",
    "### Why do we do this?\n",
    "1. To understand the data better.\n",
    "2. To understand the pattern in the variables\n",
    "3. To remove noise from the data. e.g. Missing values, outliers, irregular and unnecessary values from the data.\n",
    "\n",
    "It gives more accurate results and useful to choose machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92881dc7-594e-4188-9ed2-6b51f2b7db4f",
   "metadata": {},
   "source": [
    "### Understanding the data\n",
    "1. Univariate analysis\n",
    "    - Histogram(mostly to check if data is normally distributed or not)\n",
    "3. Bivariate analysis\n",
    "    - It contain scatter plot(If both are Numeric variables) and Box plot(if one is numeric and other is categorical variable)\n",
    "4. Multivariate analysis\n",
    "    - It Contain correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f65a82e-8207-4eae-9830-3420f2a65638",
   "metadata": {},
   "source": [
    "### Libraries needed for EDA\n",
    "\n",
    "1. Numpy: This library is used to do numeric operations on data.\n",
    "2. Pandas: This library is used to manipulate the data. e.g. importing data,basic information of data,treat Missing values and Outlier\n",
    "3. Matplotlib and seaborn: Both libraries are used for data visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456b7678-1edb-4ab3-a134-dd2622162547",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "### Handling Missing values\n",
    "1. Delete rows/column from the data:\n",
    "   If the column contain more than 50% missing values then we can delete the column.\n",
    "   If Row contain more than 2+ missing values then we can delete the row.\n",
    "2. Replace by mean or median and mode:\n",
    "   If the variable is numeric then missing values can be replaced by mean or median\n",
    "   If the variable is categorical then missing values can be replaced by mode\n",
    "3. Replacing by ML algorithms:\n",
    "   We can predict the missing values by using ML algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4e6c88-d40c-48f0-a420-926b311fbd67",
   "metadata": {},
   "source": [
    "### Handling Outliers\n",
    "#### Identify\n",
    "Outliers are identified by using Z test, IQR or visualization techniques\n",
    "- Inter quartile range:\n",
    "   IQR=Q3-Q1\n",
    "  If the data point is below Q1-1.5*IQR or greater than Q3+1.5*IQR then that data point is outlier.\n",
    "- Scatter Plot:\n",
    "    If the datapoint is away from rest of the data points then it is outlier\n",
    "- Z Score:\n",
    "    ‘Z score’ tells how many standard deviations away a data point is from the mean.\n",
    "     Z Score= (x-mean)/std\n",
    "  if z score is greater than 3 then the value is outlier.\n",
    "\n",
    "#### Handle\n",
    "1. Understand the reason for Outlier:\n",
    "   The reason might be human error or special record\n",
    "\n",
    "2. Delete outliers: Deleting is the easy way to handle outliers. But if there are many outliers then deleting them will lead to information loss.\n",
    "\n",
    "3. Winsorization: Replacing the extream values with less extream values.\n",
    "\n",
    "4. Use robust ML models: Use ML models which are less affected by outliers. e.g Decision Tree, Random forest etc.\n",
    "\n",
    "5. Data normalization and standardization: standardize or normalize the data to convert the values between perticular range so that effect of outlier will be reduced.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4df613-6824-499b-9210-33bc5c6839c2",
   "metadata": {},
   "source": [
    "### Converting Categorical variables to numeric\n",
    "1. If the categorical variable ordinal variable e.g. Rank in class then use \"Ordinal encoding\" technique.\n",
    "2. If the categeorical variable is nominal e.g. Name of colors then use \"One hot encoding\" technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0123de7b-5c85-4813-8256-257f40e1aaf2",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "- Filter methods: In this method we calculate variance of the feature. If the variance is high that means that variable contain more information\n",
    "1. Chi Square Test:\n",
    "    This test is used for categorical variables.We calculate Chi-square between each feature and the target and select the desired number of  features with the best Chi-square scores.\n",
    "\n",
    "2. Correlation Coefficient:\n",
    "   The correlation between independent variable and target variable should be high. But independent variables should be uncorrelated to each other. If there are 2 variables with high correlation then model just need 1 variable of them.\n",
    "\n",
    "- Wrapper Methods:\n",
    "1. Forward Feature Selection:\n",
    "   This method starts with no features. This is an iterative method wherein we start with the best performing variable against the target. Next, we select another variable that gives the best performance in combination with the first selected variable.\n",
    "\n",
    "2. Backward Feature Elimination:\n",
    "   This method starts with all features and eliminate the feature with less importance.\n",
    "\n",
    "- Embedded Methods:\n",
    "1. Lasso Regression:\n",
    "   It is used to reduce over fitting of the model. overfitting is reduced by penalizing the coefficients of variables. In lasso regression only the coefficient is shrink to 0.\n",
    "2. Random Forest Importance:\n",
    "   Random forest is bagging algorithm that aggregates the decision of multiple decision trees.The note feature is selected on basis of its impurity. so that is how important features can be extracted by pruning the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b357b1-02f4-4438-9fe2-3b842672b7bf",
   "metadata": {},
   "source": [
    "### Imbalanced Data\n",
    "Normally if data of 1 class have 80% of observations and other have 20% of observation then we can say that the data is Imbalanced data. In that case accuracy is not the best measure to judge the model.Using presion or recall or F1 score is the best way for model validation.\n",
    "\n",
    "#### Ways to handle imbalanced data:\n",
    "\n",
    "1. Upsampling: This process will add duplicate samples to the lower sampled class. This may see the increase in sample but the model do not learn more from that data.\n",
    "\n",
    "2. Downsampling: This process will delete random data from the high sampled data . This may cause information loss.\n",
    "\n",
    "3. SMOTE: This means Synthetic Minority Oversampling Technique. This is method of upsampling. But In this type duplicated records are not added to the lower class. The synthetic samples are created by using KNN. This is more popular method. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f52e3d8-7b93-4896-8741-38474403c31c",
   "metadata": {},
   "source": [
    "### Overfitting and Underfitting\n",
    "\n",
    "- $Overfitting$: It means the model try to learn more than required from the training data and learn the inaccurate points and errors. This have high variance i.e. The model works well with training dataset but fail on test data and low bias i.e. prediction of test data.\n",
    "\n",
    "To deal with this issue\n",
    "1. Cross-Validation\r",
    "2. \n",
    "Training with more data3. \r\n",
    "Removing feature5. s\r\n",
    "Early stopping the traini6. ng\r\n",
    "Regularizat7. ion\r\n",
    "Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6c8095-9fe7-43c2-bc1a-4affc445c406",
   "metadata": {},
   "source": [
    "#### Regularization:\n",
    "- Lasso Regression\r\n",
    "A regression model which uses the L1 Regularization technique is called LASSO(Least Absolute Shrinkage and Selection Operator) regression. Lasso Regression adds the “absolute value of magnitude” of the coefficient as a penalty term to the loss function(L). Lasso regression also helps us achieve feature selection by penalizing the weights to approximately equal to zero if that feature does not serve any purpose in the model.\r",
    "- Ridge Regression\r\n",
    "A regression model that uses the L2 regularization technique is called Ridge regression. Ridge regression adds the “squared magnitude” of the coefficient as a penalty term to the loss function(L). \r\n",
    "y \r\n",
    "i\r\n",
    "​\r\n",
    " \r\n",
    "^\r\n",
    "​\r\n",
    " ) \r\n",
    "2\r\n",
    " +λ∑ \r\n",
    "i=1\r\n",
    "m\r\n",
    "​\r\n",
    " ∣w \r\n",
    "i\r\n",
    "​\r\n",
    " ∣"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b91fad8-1eb6-452c-862b-a1f48b9aa9d2",
   "metadata": {},
   "source": [
    "- $UnderFitting$\n",
    "It is high bias and low variance.\n",
    "Reason for underfitting:\n",
    "1. Data not cleaned\n",
    "2. Size of training data is small\n",
    "3. Model is too simple\n",
    "How to tackle this issue\n",
    "1. clean the data\n",
    "2. increase the training size\n",
    "3. make model complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a6d084-e6d4-489e-9346-fa432c78eafd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbfa3b2-80f5-4ff4-be07-95826b1bc32e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
